{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 592 entries, 1963-01-01 to 2012-04-01\n",
      "Data columns (total 20 columns):\n",
      " #   Column                                                                     Non-Null Count  Dtype  \n",
      "---  ------                                                                     --------------  -----  \n",
      " 0   MSACSR                                                                     592 non-null    float64\n",
      " 1   MSPUS                                                                      592 non-null    float64\n",
      " 2   S&P Comp.                                                                  592 non-null    float64\n",
      " 3   Dividend                                                                   592 non-null    float64\n",
      " 4   Earnings                                                                   592 non-null    float64\n",
      " 5   Consumer Price Index CPI                                                   592 non-null    float64\n",
      " 6   Long Interest Rate GS10                                                    592 non-null    float64\n",
      " 7   Real Price                                                                 592 non-null    float64\n",
      " 8   Real Dividend                                                              592 non-null    float64\n",
      " 9   Real Total Return Price                                                    592 non-null    float64\n",
      " 10  Real Earnings                                                              592 non-null    float64\n",
      " 11  Real TR Scaled Earnings                                                    592 non-null    float64\n",
      " 12  Cyclically Adjusted Price Earnings Ratio P/E10 or CAPE                     592 non-null    float64\n",
      " 13  Cyclically Adjusted Total Return Price Earnings Ratio TR P/E10 or TR CAPE  592 non-null    float64\n",
      " 14  Excess CAPE Yield                                                          592 non-null    float64\n",
      " 15  Monthly Total Bond Returns                                                 592 non-null    float64\n",
      " 16  Real Total Bond Returns                                                    592 non-null    float64\n",
      " 17  10 Year Annualized Stock Real Return                                       592 non-null    float64\n",
      " 18  10 Year Annualized Bonds  Real Return                                      592 non-null    float64\n",
      " 19  Real 10 Year Excess Annualized  Returns                                    592 non-null    float64\n",
      "dtypes: float64(20)\n",
      "memory usage: 97.1 KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('data/processed.pkl')\n",
    "df = df.set_index('DATE')  # Index for timeseries is datetime\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(592, 19) (592, 1)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('MSPUS', axis='columns')\n",
    "y = df['MSPUS'].values\n",
    "y = y.reshape(-1, 1)  # Put target variable into column vector shape\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([532, 1, 19])\n",
      "torch.Size([60, 1, 19])\n"
     ]
    }
   ],
   "source": [
    "# Make train and validation sets\n",
    "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# Convert datasets to nodes in computational graph\n",
    "X_train_tensors = torch.Tensor(X_train.values)\n",
    "y_train_tensors = torch.Tensor(y_train)\n",
    "X_val_tensors = torch.Tensor(X_val.values)\n",
    "y_val_tensors = torch.Tensor(y_val)\n",
    "\n",
    "# Convert to sequential data for pytorch\n",
    "X_train_tensors = torch.reshape(X_train_tensors, (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n",
    "X_val_tensors = torch.reshape(X_val_tensors, (X_val_tensors.shape[0], 1, X_val_tensors.shape[1]))\n",
    "\n",
    "print(X_train_tensors.shape)\n",
    "print(X_val_tensors.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(input_size = input_size,\n",
    "                            hidden_size = hidden_size,\n",
    "                            num_layers = num_layers,\n",
    "                            batch_first=True\n",
    "                           )\n",
    "        self.fully_connected1 = torch.nn.Linear(hidden_size, 128)\n",
    "        self.fully_connected2 = torch.nn.Linear(128, num_classes)\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize states\n",
    "        initial_hidden_state   = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        initial_internal_state = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        \n",
    "        # Propogate forward\n",
    "        output, (hidden_state, internal_state) = self.lstm(x, (initial_hidden_state, initial_internal_state))\n",
    "        hidden_state = hidden_state.view(-1, self.hidden_size)  # Reshape for next dense layer\n",
    "        output = self.relu(hidden_state)\n",
    "        output = self.fully_connected1(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.fully_connected2(output)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000, loss: 12627835904.00\n",
      "Epoch: 2000, loss: 6801827840.00\n",
      "Epoch: 3000, loss: 5279175168.00\n",
      "Epoch: 4000, loss: 5229205504.00\n",
      "Epoch: 5000, loss: 5229145088.00\n",
      "Epoch: 6000, loss: 5229144064.00\n",
      "Epoch: 7000, loss: 5229144064.00\n",
      "Epoch: 8000, loss: 5229144064.00\n",
      "Epoch: 9000, loss: 5229144064.00\n",
      "Epoch: 10000, loss: 5229144064.00\n",
      "Epoch: 11000, loss: 5229144064.00\n",
      "Epoch: 12000, loss: 5229144064.00\n",
      "Epoch: 13000, loss: 5229144576.00\n",
      "Epoch: 14000, loss: 5229144576.00\n",
      "Epoch: 15000, loss: 5229144576.00\n",
      "Epoch: 16000, loss: 5229144064.00\n",
      "Epoch: 17000, loss: 5229144064.00\n",
      "Epoch: 18000, loss: 5229144064.00\n",
      "Epoch: 19000, loss: 5229144576.00\n",
      "Epoch: 20000, loss: 5229144064.00\n",
      "Epoch: 21000, loss: 5229144064.00\n",
      "Epoch: 22000, loss: 5229144576.00\n",
      "Epoch: 23000, loss: 5229143552.00\n",
      "Epoch: 24000, loss: 5229144064.00\n",
      "Epoch: 25000, loss: 5229144064.00\n",
      "Epoch: 26000, loss: 5229144064.00\n",
      "Epoch: 27000, loss: 5229144064.00\n",
      "Epoch: 28000, loss: 5229144064.00\n",
      "Epoch: 29000, loss: 5229143552.00\n",
      "Epoch: 30000, loss: 5229144064.00\n",
      "Epoch: 31000, loss: 5229144576.00\n",
      "Epoch: 32000, loss: 5229143040.00\n",
      "Epoch: 33000, loss: 5229144576.00\n",
      "Epoch: 34000, loss: 5229144064.00\n",
      "Epoch: 35000, loss: 5229144064.00\n",
      "Epoch: 36000, loss: 5229144064.00\n",
      "Epoch: 37000, loss: 5229144576.00\n",
      "Epoch: 38000, loss: 5229144576.00\n",
      "Epoch: 39000, loss: 5229144576.00\n",
      "Epoch: 40000, loss: 5229143552.00\n",
      "Epoch: 41000, loss: 5229144576.00\n",
      "Epoch: 42000, loss: 5229144064.00\n",
      "Epoch: 43000, loss: 5229144064.00\n",
      "Epoch: 44000, loss: 5229144064.00\n",
      "Epoch: 45000, loss: 5229144064.00\n",
      "Epoch: 46000, loss: 5229144064.00\n",
      "Epoch: 47000, loss: 5229144064.00\n",
      "Epoch: 48000, loss: 5229144064.00\n",
      "Epoch: 49000, loss: 5229144064.00\n",
      "Epoch: 50000, loss: 5229144576.00\n",
      "Epoch: 51000, loss: 5229144064.00\n",
      "Epoch: 52000, loss: 5229143552.00\n",
      "Epoch: 53000, loss: 5229144576.00\n",
      "Epoch: 54000, loss: 5229144064.00\n",
      "Epoch: 55000, loss: 5229144064.00\n",
      "Epoch: 56000, loss: 5229144064.00\n",
      "Epoch: 57000, loss: 5229144064.00\n",
      "Epoch: 58000, loss: 5229144064.00\n",
      "Epoch: 59000, loss: 5229144064.00\n",
      "Epoch: 60000, loss: 5229144064.00\n",
      "Epoch: 61000, loss: 5229144064.00\n",
      "Epoch: 62000, loss: 5229144064.00\n",
      "Epoch: 63000, loss: 5229143552.00\n",
      "Epoch: 64000, loss: 5229144064.00\n",
      "Epoch: 65000, loss: 5229144064.00\n",
      "Epoch: 66000, loss: 5229144064.00\n",
      "Epoch: 67000, loss: 5229144576.00\n",
      "Epoch: 68000, loss: 5229144064.00\n",
      "Epoch: 69000, loss: 5229144064.00\n",
      "Epoch: 70000, loss: 5229144064.00\n",
      "Epoch: 71000, loss: 5229144064.00\n",
      "Epoch: 72000, loss: 5229144064.00\n",
      "Epoch: 73000, loss: 5229144064.00\n",
      "Epoch: 74000, loss: 5229144064.00\n",
      "Epoch: 75000, loss: 5229144064.00\n",
      "Epoch: 76000, loss: 5229143552.00\n",
      "Epoch: 77000, loss: 5229144064.00\n",
      "Epoch: 78000, loss: 5229144064.00\n",
      "Epoch: 79000, loss: 5229144064.00\n",
      "Epoch: 80000, loss: 5229144064.00\n",
      "Epoch: 81000, loss: 5229144576.00\n",
      "Epoch: 82000, loss: 5229144576.00\n",
      "Epoch: 83000, loss: 5229144064.00\n",
      "Epoch: 84000, loss: 5229144064.00\n",
      "Epoch: 85000, loss: 5229144064.00\n",
      "Epoch: 86000, loss: 5229144064.00\n",
      "Epoch: 87000, loss: 5229144064.00\n",
      "Epoch: 88000, loss: 5229144064.00\n",
      "Epoch: 89000, loss: 5229144064.00\n",
      "Epoch: 90000, loss: 5229144064.00\n",
      "Epoch: 91000, loss: 5229143552.00\n",
      "Epoch: 92000, loss: 5229144064.00\n",
      "Epoch: 93000, loss: 5229144064.00\n",
      "Epoch: 94000, loss: 5229144064.00\n",
      "Epoch: 95000, loss: 5229144576.00\n",
      "Epoch: 96000, loss: 5229144064.00\n",
      "Epoch: 97000, loss: 5229144064.00\n",
      "Epoch: 98000, loss: 5229144064.00\n",
      "Epoch: 99000, loss: 5229144576.00\n",
      "Epoch: 100000, loss: 5229144064.00\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "N_CLASSES = 1\n",
    "INPUT_SIZE = X_train_tensors.shape[2]  # Number of features\n",
    "HIDDEN_SIZE = 2 # Number of features in hidden state\n",
    "N_LAYERS = 1  # Number of stacked LSTM layers\n",
    "BIAS = X_train_tensors.shape[1]\n",
    "\n",
    "# Training parameters\n",
    "N_EPOCHS = 100000\n",
    "LEARNING_RATE = 0.01\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "lstm = LSTM(N_CLASSES, INPUT_SIZE, HIDDEN_SIZE, N_LAYERS, BIAS)\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=LEARNING_RATE)\n",
    "for epoch_i in range(1, N_EPOCHS+1):\n",
    "    # Forward propogation\n",
    "    outputs = lstm.forward(X_train_tensors)\n",
    "    optimizer.zero_grad()  # Manually set gradient to 0 here\n",
    "    \n",
    "    # Backward propogation\n",
    "    np_outputs = outputs.detach().numpy()\n",
    "    #if np.all(np_outputs == np_outputs[0]):\n",
    "        #print('Epoch {}: All outputs the same'.format(epoch_i))\n",
    "    loss = loss_func(outputs, y_train_tensors)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    if epoch_i % 1000 == 0:\n",
    "        print('Epoch: {}, loss: {:.2f}'.format(epoch_i, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_predictions = lstm(X_val_tensors).data.numpy()  # Forward pass\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "#plt.plot(y_val, label='Actual')\n",
    "plt.plot(val_predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(122947.7812)\n"
     ]
    }
   ],
   "source": [
    "print(y_val_tensors.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mas-thesis",
   "language": "python",
   "name": "mas-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
