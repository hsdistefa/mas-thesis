{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harrison/anaconda3/envs/mas-thesis/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 592 entries, 1963-01-01 to 2012-04-01\n",
      "Data columns (total 20 columns):\n",
      " #   Column                                                                     Non-Null Count  Dtype  \n",
      "---  ------                                                                     --------------  -----  \n",
      " 0   MSACSR                                                                     592 non-null    float64\n",
      " 1   MSPUS                                                                      592 non-null    float64\n",
      " 2   S&P Comp.                                                                  592 non-null    float64\n",
      " 3   Dividend                                                                   592 non-null    float64\n",
      " 4   Earnings                                                                   592 non-null    float64\n",
      " 5   Consumer Price Index CPI                                                   592 non-null    float64\n",
      " 6   Long Interest Rate GS10                                                    592 non-null    float64\n",
      " 7   Real Price                                                                 592 non-null    float64\n",
      " 8   Real Dividend                                                              592 non-null    float64\n",
      " 9   Real Total Return Price                                                    592 non-null    float64\n",
      " 10  Real Earnings                                                              592 non-null    float64\n",
      " 11  Real TR Scaled Earnings                                                    592 non-null    float64\n",
      " 12  Cyclically Adjusted Price Earnings Ratio P/E10 or CAPE                     592 non-null    float64\n",
      " 13  Cyclically Adjusted Total Return Price Earnings Ratio TR P/E10 or TR CAPE  592 non-null    float64\n",
      " 14  Excess CAPE Yield                                                          592 non-null    float64\n",
      " 15  Monthly Total Bond Returns                                                 592 non-null    float64\n",
      " 16  Real Total Bond Returns                                                    592 non-null    float64\n",
      " 17  10 Year Annualized Stock Real Return                                       592 non-null    float64\n",
      " 18  10 Year Annualized Bonds  Real Return                                      592 non-null    float64\n",
      " 19  Real 10 Year Excess Annualized  Returns                                    592 non-null    float64\n",
      "dtypes: float64(20)\n",
      "memory usage: 97.1 KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('data/processed.pkl')\n",
    "df = df.set_index('DATE')  # Index for timeseries is datetime\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Data interpolation if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(592, 19)\n",
      "(592, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split into input and target variables\n",
    "\n",
    "X = df.drop('MSPUS', axis='columns')\n",
    "y = df['MSPUS'].values\n",
    "y = y.reshape(-1, 1)  # Put target variable into column vector shape\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X mean: -3.916576246188319e-16\n",
      "X var: 1.0\n",
      "y min: 0.0\n",
      "y max: 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "# Scale data\n",
    "\n",
    "standard_scaler = sklearn.preprocessing.StandardScaler()  # Scales to mean of 0 and var of 1\n",
    "minmax_scaler = sklearn.preprocessing.MinMaxScaler()  # Scale to between 0 and 1\n",
    "\n",
    "X_scaled = standard_scaler.fit_transform(X)  \n",
    "y_scaled = minmax_scaler.fit_transform(y)    \n",
    "\n",
    "print(\"X mean:\", X_scaled.mean())\n",
    "print(\"X var:\", X_scaled.var())\n",
    "print(\"y min:\", y_scaled.min())\n",
    "print(\"y max:\", y_scaled.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(579, 10, 19) (579, 5)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: These values are for monthly data\n",
    "#       Change these if interpolating!\n",
    "IN_SEQ_LENGTH = 10\n",
    "OUT_SEQ_LENGTH = 5\n",
    "\n",
    "def split_sequences(input_sequences, output_sequence, n_steps_in, n_steps_out):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(input_sequences)):\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out - 1\n",
    "        if out_end_ix > len(input_sequences):\n",
    "            break\n",
    "        seq_x, seq_y = input_sequences[i:end_ix], output_sequence[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x), y.append(seq_y)\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_seq, y_seq = split_sequences(X_scaled, y_scaled, IN_SEQ_LENGTH, OUT_SEQ_LENGTH)\n",
    "\n",
    "# Make \n",
    "print(X_seq.shape, y_seq.shape)\n",
    "\n",
    "assert y_seq[0].all() == y_scaled[IN_SEQ_LENGTH-1:IN_SEQ_LENGTH-1+OUT_SEQ_LENGTH].squeeze(1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([550, 10, 19])\n",
      "torch.Size([29, 10, 19])\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X_seq, y_seq, test_size=0.05)\n",
    "\n",
    "# Convert datasets to nodes in computational graph\n",
    "X_train_tensors = torch.Tensor(X_train)\n",
    "y_train_tensors = torch.Tensor(y_train)\n",
    "X_val_tensors = torch.Tensor(X_val)\n",
    "y_val_tensors = torch.Tensor(y_val)\n",
    "\n",
    "# Convert to sequential data for pytorch\n",
    "X_train_tensors = torch.reshape(X_train_tensors, \n",
    "                                (X_train_tensors.shape[0], IN_SEQ_LENGTH, X_train_tensors.shape[2])\n",
    "                               )\n",
    "X_val_tensors = torch.reshape(X_val_tensors, (X_val_tensors.shape[0], IN_SEQ_LENGTH, X_val_tensors.shape[2]))\n",
    "\n",
    "\n",
    "print(X_train_tensors.shape)\n",
    "print(X_val_tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.2000000e+00 1.3044900e+03 2.3430000e+01 8.1310000e+01 2.2347000e+02\n",
      "  3.4100000e+00 1.6948900e+03 3.0440000e+01 8.8240620e+05 1.0564000e+02\n",
      "  5.5001150e+04 2.2900000e+01 2.5270000e+01 3.3600000e+00 1.0000000e+00\n",
      "  4.3690000e+01 1.1900000e+01 1.9500000e+00 9.9500000e+00]\n",
      " [6.7000000e+00 1.3315100e+03 2.3730000e+01 8.2160000e+01 2.2491000e+02\n",
      "  3.4600000e+00 1.7189300e+03 3.0640000e+01 8.9625005e+05 1.0607000e+02\n",
      "  5.5304800e+04 2.3140000e+01 2.5530000e+01 3.2900000e+00 1.0300000e+00\n",
      "  4.3350000e+01 1.2290000e+01 1.9400000e+00 1.0360000e+01]\n",
      " [6.6000000e+00 1.3383100e+03 2.4040000e+01 8.3020000e+01 2.2596000e+02\n",
      "  3.1700000e+00 1.7196200e+03 3.0890000e+01 8.9795133e+05 1.0667000e+02\n",
      "  5.5700790e+04 2.3060000e+01 2.5420000e+01 3.6000000e+00 1.0200000e+00\n",
      "  4.4330000e+01 1.2270000e+01 1.6600000e+00 1.0610000e+01]\n",
      " [6.6000000e+00 1.2872900e+03 2.4340000e+01 8.3870000e+01 2.2572000e+02\n",
      "  3.0000000e+00 1.6558300e+03 3.1310000e+01 8.6600739e+05 1.0788000e+02\n",
      "  5.6422440e+04 2.2100000e+01 2.4360000e+01 3.9300000e+00 1.0000000e+00\n",
      "  4.5140000e+01 1.2770000e+01 1.4900000e+00 1.1290000e+01]]\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check data\n",
    "X_check, y_checkk = split_sequences(X, y.reshape(-1, 1), IN_SEQ_LENGTH, OUT_SEQ_LENGTH)\n",
    "print(X_check[-1][0:4])\n",
    "\n",
    "start_ix = IN_SEQ_LENGTH + OUT_SEQ_LENGTH\n",
    "\n",
    "#print(X.iloc[-start_ix + 1: -start_ix + 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, n_classes, n_inputs, n_hidden, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes  # Output size\n",
    "        self.n_inputs = n_inputs  # Input size\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers  # Number of reccurrent layers\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(input_size=n_inputs, \n",
    "                                  hidden_size=n_hidden,\n",
    "                                  num_layers=n_layers,\n",
    "                                  batch_first=True,  # Input/Output layers are of the form (batch, seq, feature)\n",
    "                                  dropout=0.2  # Dropout helps to avoid over-fitting/improves robustness\n",
    "                                 )\n",
    "        self.fc1 = torch.nn.Linear(n_hidden, 128)  # Fully connected layer 1\n",
    "        self.fc2 = torch.nn.Linear(128, n_classes)  # Fully connected layer 2\n",
    "        self.relu = torch.nn.ReLU()  # Activation layer\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Initialize hidden state\n",
    "        h_0 = torch.Tensor(torch.zeros(self.n_layers, X.size(0), self.n_hidden))\n",
    "        # Initialize cell state\n",
    "        c_0 = torch.Tensor(torch.zeros(self.n_layers, X.size(0), self.n_hidden))\n",
    "        \n",
    "        # Forward propogate inputs\n",
    "        output, (h_n, c_n) = self.lstm(X, (h_0, c_0))\n",
    "        h_n = h_n.view(-1, self.n_hidden)\n",
    "        out = self.relu(h_n)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "\n",
    "def training_loop(n_epochs, lstm, optimizer, loss_fn, X_train, y_train, X_test, y_test):\n",
    "    for epoch_i in range(n_epochs):\n",
    "        lstm.train()\n",
    "        outputs = lstm.forward(X_train)  # Forward pass\n",
    "        optimizer.zero_grad()  # Calculate gradient, manually set to 0\n",
    "        loss = loss_fn(outputs, y_train)\n",
    "        loss.backward()  # Calculate loss\n",
    "        optimizer.step()  # Backpropogate loss\n",
    "\n",
    "        # Test set loss\n",
    "        lstm.eval()\n",
    "        test_preds = lstm(X_test)\n",
    "        test_loss = loss_fn(test_preds, y_test)\n",
    "        if epoch_i % 100 == 0:\n",
    "            print(\"Epoch {}: Train loss: {:.8f} Test loss: {:.15f}\".format(epoch_i, \n",
    "                                                                          loss.item(),\n",
    "                                                                          test_loss.item()\n",
    "                                                                         )\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss: 0.24628602 Test loss: 0.257878571748734\n",
      "Epoch 100: Train loss: 0.00212084 Test loss: 0.002428826177493\n",
      "Epoch 200: Train loss: 0.00055869 Test loss: 0.000520960311405\n",
      "Epoch 300: Train loss: 0.00028580 Test loss: 0.000250963144936\n",
      "Epoch 400: Train loss: 0.00019334 Test loss: 0.000197241577553\n",
      "Epoch 500: Train loss: 0.00014663 Test loss: 0.000171701161889\n",
      "Epoch 600: Train loss: 0.00011695 Test loss: 0.000152202715981\n",
      "Epoch 700: Train loss: 0.00010127 Test loss: 0.000132934612338\n",
      "Epoch 800: Train loss: 0.00009008 Test loss: 0.000117096147733\n",
      "Epoch 900: Train loss: 0.00008199 Test loss: 0.000104910955997\n",
      "Epoch 1000: Train loss: 0.00007595 Test loss: 0.000093352115073\n",
      "Epoch 1100: Train loss: 0.00007108 Test loss: 0.000083842271124\n",
      "Epoch 1200: Train loss: 0.00006694 Test loss: 0.000076691045251\n",
      "Epoch 1300: Train loss: 0.00006331 Test loss: 0.000070943940955\n",
      "Epoch 1400: Train loss: 0.00006019 Test loss: 0.000066774577135\n",
      "Epoch 1500: Train loss: 0.00005742 Test loss: 0.000064534448029\n",
      "Epoch 1600: Train loss: 0.00005507 Test loss: 0.000062109298597\n",
      "Epoch 1700: Train loss: 0.00005295 Test loss: 0.000059611807956\n",
      "Epoch 1800: Train loss: 0.00005107 Test loss: 0.000057852808823\n",
      "Epoch 1900: Train loss: 0.00004932 Test loss: 0.000056471984863\n",
      "Epoch 2000: Train loss: 0.00004762 Test loss: 0.000055621527281\n",
      "Epoch 2100: Train loss: 0.00004585 Test loss: 0.000053102517995\n",
      "Epoch 2200: Train loss: 0.00004423 Test loss: 0.000050651400670\n",
      "Epoch 2300: Train loss: 0.00004279 Test loss: 0.000049878934078\n",
      "Epoch 2400: Train loss: 0.00004148 Test loss: 0.000048533507652\n",
      "Epoch 2500: Train loss: 0.00004027 Test loss: 0.000047378682211\n"
     ]
    }
   ],
   "source": [
    "# Run model\n",
    "\n",
    "\n",
    "# Define model parameters\n",
    "N_EPOCHS = 10000\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "INPUT_SIZE = X.shape[1]  # Number of features\n",
    "HIDDEN_SIZE = 10  # Number of features in hidden state\n",
    "N_LAYERS = 1 # Number of stacked lstm layers\n",
    "\n",
    "N_CLASSES = OUT_SEQ_LENGTH  # Equal to how many timesteps in future we want to predict\n",
    "\n",
    "lstm1 = LSTM(N_CLASSES,\n",
    "            INPUT_SIZE,\n",
    "            HIDDEN_SIZE,\n",
    "            N_LAYERS\n",
    "           )\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lstm1.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Run training loop\n",
    "training_loop(n_epochs=N_EPOCHS,\n",
    "              lstm=lstm1,\n",
    "              optimizer=optimizer,\n",
    "              loss_fn=loss_fn,\n",
    "              X_train=X_train_tensors,\n",
    "              y_train=y_train_tensors,\n",
    "              X_test=X_val_tensors,\n",
    "              y_test=y_val_tensors\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mas-thesis",
   "language": "python",
   "name": "mas-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
