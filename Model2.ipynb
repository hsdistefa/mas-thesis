{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harrison/anaconda3/envs/mas-thesis/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 592 entries, 1963-01-01 to 2012-04-01\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   MSACSR                      592 non-null    float64\n",
      " 1   MSPUS                       592 non-null    float64\n",
      " 2   S&P Comp.                   592 non-null    float64\n",
      " 3   Dividend                    592 non-null    float64\n",
      " 4   Earnings                    592 non-null    float64\n",
      " 5   CPI                         592 non-null    float64\n",
      " 6   Long Interest Rate GS10     592 non-null    float64\n",
      " 7   Real Price                  592 non-null    float64\n",
      " 8   Real Dividend               592 non-null    float64\n",
      " 9   Real Total Return Price     592 non-null    float64\n",
      " 10  Real Earnings               592 non-null    float64\n",
      " 11  Real TR Scaled Earnings     592 non-null    float64\n",
      " 12  CAPE                        592 non-null    float64\n",
      " 13  Total Return CAPE           592 non-null    float64\n",
      " 14  Excess CAPE Yield           592 non-null    float64\n",
      " 15  Monthly Total Bond Returns  592 non-null    float64\n",
      " 16  Real Total Bond Returns     592 non-null    float64\n",
      " 17  10 Year Stock Real Return   592 non-null    float64\n",
      " 18  10 Year Bonds Real Return   592 non-null    float64\n",
      " 19  10 Year Excess Returns      592 non-null    float64\n",
      " 20  null_locations              592 non-null    object \n",
      " 21  null_count                  592 non-null    int64  \n",
      " 22  Year                        592 non-null    int64  \n",
      " 23  logdiff_MSPUS               591 non-null    float64\n",
      "dtypes: float64(21), int64(2), object(1)\n",
      "memory usage: 115.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_pickle('data/processed.pkl')\n",
    "df_raw = df_raw.set_index('DATE')  # Index for timeseries is datetime\n",
    "\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Data interpolation if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(591, 21)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 591 entries, 1963-02-01 to 2012-04-01\n",
      "Data columns (total 21 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   MSACSR                      591 non-null    float64\n",
      " 1   S&P Comp.                   591 non-null    float64\n",
      " 2   Dividend                    591 non-null    float64\n",
      " 3   Earnings                    591 non-null    float64\n",
      " 4   CPI                         591 non-null    float64\n",
      " 5   Long Interest Rate GS10     591 non-null    float64\n",
      " 6   Real Price                  591 non-null    float64\n",
      " 7   Real Dividend               591 non-null    float64\n",
      " 8   Real Total Return Price     591 non-null    float64\n",
      " 9   Real Earnings               591 non-null    float64\n",
      " 10  Real TR Scaled Earnings     591 non-null    float64\n",
      " 11  CAPE                        591 non-null    float64\n",
      " 12  Total Return CAPE           591 non-null    float64\n",
      " 13  Excess CAPE Yield           591 non-null    float64\n",
      " 14  Monthly Total Bond Returns  591 non-null    float64\n",
      " 15  Real Total Bond Returns     591 non-null    float64\n",
      " 16  10 Year Stock Real Return   591 non-null    float64\n",
      " 17  10 Year Bonds Real Return   591 non-null    float64\n",
      " 18  10 Year Excess Returns      591 non-null    float64\n",
      " 19  logdiff_MSPUS               591 non-null    float64\n",
      " 20  target                      591 non-null    float64\n",
      "dtypes: float64(21)\n",
      "memory usage: 101.6 KB\n"
     ]
    }
   ],
   "source": [
    "df = df_raw.copy()\n",
    "\n",
    "# Set target variable\n",
    "df['target'] = df_raw['logdiff_MSPUS']\n",
    "\n",
    "# Remove extraneous columns\n",
    "df = df.drop(['null_locations', 'null_count', 'Year', 'MSPUS'], axis='columns')\n",
    "\n",
    "# Remove first row since null for differenced columns\n",
    "df = df.drop(index=df.index[0], axis='index')\n",
    "\n",
    "print(df.shape)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(591, 20)\n",
      "(591, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split into input and target variables\n",
    "\n",
    "X = df.drop('target', axis='columns')\n",
    "y = df['target'].values\n",
    "y = y.reshape(-1, 1)  # Put target variable into column vector shape\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X mean: -6.462211852302096e-16\n",
      "X var: 1.0\n",
      "y min: 0.0\n",
      "y max: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Scale data\n",
    "\n",
    "standard_scaler = sklearn.preprocessing.StandardScaler()  # Scales to mean of 0 and var of 1\n",
    "minmax_scaler = sklearn.preprocessing.MinMaxScaler()  # Scale to between 0 and 1\n",
    "\n",
    "X_scaled = standard_scaler.fit_transform(X)  \n",
    "y_scaled = minmax_scaler.fit_transform(y)    \n",
    "\n",
    "print(\"X mean:\", X_scaled.mean())\n",
    "print(\"X var:\", X_scaled.var())\n",
    "print(\"y min:\", y_scaled.min())\n",
    "print(\"y max:\", y_scaled.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(557, 24, 20) (557, 12)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: These values are for monthly data\n",
    "#       Change these if interpolating!\n",
    "IN_SEQ_LENGTH = 24\n",
    "OUT_SEQ_LENGTH = 12\n",
    "\n",
    "def split_sequences(input_sequences, output_sequence, n_steps_in, n_steps_out):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(input_sequences)):\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out - 1\n",
    "        if out_end_ix > len(input_sequences):\n",
    "            break\n",
    "        seq_x, seq_y = input_sequences[i:end_ix], output_sequence[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x), y.append(seq_y)\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_seq, y_seq = split_sequences(X_scaled, y_scaled, IN_SEQ_LENGTH, OUT_SEQ_LENGTH)\n",
    "\n",
    "# Make \n",
    "print(X_seq.shape, y_seq.shape)\n",
    "\n",
    "assert y_seq[0].all() == y_scaled[IN_SEQ_LENGTH-1:IN_SEQ_LENGTH-1+OUT_SEQ_LENGTH].squeeze(1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([529, 24, 20])\n",
      "torch.Size([28, 24, 20])\n"
     ]
    }
   ],
   "source": [
    "TEST_SIZE = 0.05\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(X_seq, y_seq, test_size=TEST_SIZE)\n",
    "\n",
    "# Convert datasets to nodes in computational graph\n",
    "X_train_tensors = torch.Tensor(X_train)\n",
    "y_train_tensors = torch.Tensor(y_train)\n",
    "X_val_tensors = torch.Tensor(X_val)\n",
    "y_val_tensors = torch.Tensor(y_val)\n",
    "\n",
    "# Convert to sequential data for pytorch\n",
    "X_train_tensors = torch.reshape(X_train_tensors, \n",
    "                                (X_train_tensors.shape[0], IN_SEQ_LENGTH, X_train_tensors.shape[2])\n",
    "                               )\n",
    "X_val_tensors = torch.reshape(X_val_tensors, (X_val_tensors.shape[0], IN_SEQ_LENGTH, X_val_tensors.shape[2]))\n",
    "\n",
    "\n",
    "print(X_train_tensors.shape)\n",
    "print(X_val_tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.50000000e+00  9.26120000e+02  2.55900000e+01  7.51000000e+00\n",
      "   2.15690000e+02  3.72000000e+00  1.24665000e+03  3.44500000e+01\n",
      "   6.26478280e+05  1.01100000e+01  5.08018000e+03  1.63800000e+01\n",
      "   1.80400000e+01  5.02000000e+00  1.02000000e+00  4.16700000e+01\n",
      "   1.23700000e+01  2.17000000e+00  1.02100000e+01 -1.01103802e-02]\n",
      " [ 7.90000000e+00  9.35820000e+02  2.50300000e+01  9.19000000e+00\n",
      "   2.15350000e+02  3.56000000e+00  1.26171000e+03  3.37400000e+01\n",
      "   6.35458250e+05  1.23900000e+01  6.23810000e+03  1.66900000e+01\n",
      "   1.84100000e+01  5.02000000e+00  1.00000000e+00  4.24200000e+01\n",
      "   1.26200000e+01  2.38000000e+00  1.02400000e+01 -1.02136450e-02]\n",
      " [ 7.50000000e+00  1.00973000e+03  2.44600000e+01  1.08600000e+01\n",
      "   2.15830000e+02  3.59000000e+00  1.35831000e+03  3.29100000e+01\n",
      "   6.85492860e+05  1.46100000e+01  7.37498000e+03  1.80900000e+01\n",
      "   1.99800000e+01  4.53000000e+00  1.02000000e+00  4.23500000e+01\n",
      "   1.14100000e+01  2.41000000e+00  9.00000000e+00  7.28403066e-03]\n",
      " [ 7.80000000e+00  1.04455000e+03  2.39000000e+01  1.25400000e+01\n",
      "   2.15970000e+02  3.40000000e+00  1.40428000e+03  3.21300000e+01\n",
      "   7.10039710e+05  1.68600000e+01  8.52415000e+03  1.88300000e+01\n",
      "   2.08200000e+01  4.46000000e+00  1.00000000e+00  4.31200000e+01\n",
      "   1.13500000e+01  2.17000000e+00  9.19000000e+00  7.23135700e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check data\n",
    "X_check, y_check = split_sequences(X, y.reshape(-1, 1), IN_SEQ_LENGTH, OUT_SEQ_LENGTH)\n",
    "print(X_check[-1][0:4])\n",
    "\n",
    "start_ix = IN_SEQ_LENGTH + OUT_SEQ_LENGTH\n",
    "\n",
    "#print(X.iloc[-start_ix + 1: -start_ix + 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, n_classes, n_inputs, n_hidden, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes  # Output size\n",
    "        self.n_inputs = n_inputs  # Input size\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers  # Number of reccurrent layers\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(input_size=n_inputs, \n",
    "                                  hidden_size=n_hidden,\n",
    "                                  num_layers=n_layers,\n",
    "                                  batch_first=True,  # Input/Output layers are of the form (batch, seq, feature)\n",
    "                                  dropout=0.2  # Dropout helps to avoid over-fitting/improves robustness\n",
    "                                 )\n",
    "        self.fc1 = torch.nn.Linear(n_hidden, 128)  # Fully connected layer 1\n",
    "        self.fc2 = torch.nn.Linear(128, n_classes)  # Fully connected layer 2\n",
    "        self.relu = torch.nn.ReLU()  # Activation layer\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Initialize hidden state\n",
    "        h_0 = torch.Tensor(torch.zeros(self.n_layers, X.size(0), self.n_hidden))\n",
    "        # Initialize cell state\n",
    "        c_0 = torch.Tensor(torch.zeros(self.n_layers, X.size(0), self.n_hidden))\n",
    "        \n",
    "        # Forward propogate inputs\n",
    "        output, (h_n, c_n) = self.lstm(X, (h_0, c_0))\n",
    "        h_n = h_n.view(-1, self.n_hidden)\n",
    "        out = self.relu(h_n)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "\n",
    "def training_loop(n_epochs, lstm, optimizer, loss_fn, X_train, y_train, X_test, y_test):\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    for epoch_i in range(n_epochs):\n",
    "        lstm.train()\n",
    "        outputs = lstm.forward(X_train)  # Forward pass\n",
    "        optimizer.zero_grad()  # Calculate gradient, manually set to 0\n",
    "        loss = loss_fn(outputs, y_train)\n",
    "        loss.backward()  # Calculate loss\n",
    "        optimizer.step()  # Backpropogate loss\n",
    "\n",
    "        # Test set loss\n",
    "        lstm.eval()\n",
    "        test_preds = lstm(X_test)\n",
    "        test_loss = loss_fn(test_preds, y_test)\n",
    "        if epoch_i % 100 == 0:\n",
    "            print(\"Epoch {}: Train loss: {:.8f} Test loss: {:.15f}\".format(epoch_i, \n",
    "                                                                          loss.item(),\n",
    "                                                                          test_loss.item()\n",
    "                                                                         )\n",
    "                 )\n",
    "        train_loss_list.append(loss.item())\n",
    "        test_loss_list.append(test_loss.item())\n",
    "    \n",
    "    return train_loss_list, test_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harrison/anaconda3/envs/mas-thesis/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss: 0.33445346 Test loss: 0.340479075908661\n",
      "Epoch 100: Train loss: 0.02252129 Test loss: 0.022162824869156\n"
     ]
    }
   ],
   "source": [
    "# Run model\n",
    "\n",
    "\n",
    "# Define model parameters\n",
    "N_EPOCHS = 1000\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "INPUT_SIZE = X.shape[1]  # Number of features\n",
    "HIDDEN_SIZE = 10  # Number of features in hidden state\n",
    "N_LAYERS = 1 # Number of stacked lstm layers\n",
    "\n",
    "N_CLASSES = OUT_SEQ_LENGTH  # Equal to how many timesteps in future we want to predict\n",
    "\n",
    "lstm1 = LSTM(N_CLASSES,\n",
    "            INPUT_SIZE,\n",
    "            HIDDEN_SIZE,\n",
    "            N_LAYERS\n",
    "           )\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lstm1.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Run training loop\n",
    "train_loss, test_loss = training_loop(n_epochs=N_EPOCHS,\n",
    "              lstm=lstm1,\n",
    "              optimizer=optimizer,\n",
    "              loss_fn=loss_fn,\n",
    "              X_train=X_train_tensors,\n",
    "              y_train=y_train_tensors,\n",
    "              X_test=X_val_tensors,\n",
    "              y_test=y_val_tensors\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label=\"Train Loss\")\n",
    "plt.plot(test_loss, label=\"Test Loss\")\n",
    "plt.title(\"Model Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual\n",
    "\n",
    "def inverse_pct_change(series, y_0):\n",
    "    cumulative_sum = np.array(series).cumsum()\n",
    "    \n",
    "    return y_0 * np.exp(cumulative_sum)\n",
    "\n",
    "\n",
    "# Get data into the right format\n",
    "X_tensors = torch.Tensor(X_seq)\n",
    "y_tensors = torch.Tensor(y_seq)\n",
    "\n",
    "X_tensors = torch.reshape(X_tensors, (X_tensors.shape[0], IN_SEQ_LENGTH, X_tensors.shape[2]))\n",
    "\n",
    "# Get predictions and actual\n",
    "predictions_scaled = lstm1(X_tensors).data.numpy()  # Convert to numpy for plotting\n",
    "actual_scaled = y_tensors.data.numpy()\n",
    "\n",
    "# Reverse scaling transformations\n",
    "predictions_seq = minmax_scaler.inverse_transform(predictions_scaled)  # y values used minmax scaler earlier\n",
    "actual_seq = minmax_scaler.inverse_transform(actual_scaled)\n",
    "\n",
    "# Reverse sequence transformations\n",
    "predictions = [predictions_seq[i][0] for i in range(len(predictions_seq))]\n",
    "actual = [actual_seq[i][0] for i in range(len(actual_seq))]\n",
    "\n",
    "# Reverse log difference transformations\n",
    "first_target = 17800  # First MSPUS value before log differencing\n",
    "predictions_final = inverse_pct_change(predictions, first_target)\n",
    "actual_final = inverse_pct_change(actual, first_target)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "\n",
    "train_test_cutoff = round((1-TEST_SIZE) * X_tensors.shape[0])  # Index where test data starts\n",
    "plt.axvline(x=train_test_cutoff, c='red', linestyle='--')\n",
    "\n",
    "plt.plot(actual_final, label='Actual')\n",
    "plt.plot(predictions_final, label='Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict future based only on last in sequence length\n",
    "\n",
    "# Get last MSPUS value before start of predictions\n",
    "first_target = df_raw['MSPUS'].iloc[-(OUT_SEQ_LENGTH+1)]  \n",
    "\n",
    "# Predictions\n",
    "test_predictions = lstm1(X_val_tensors[-1].unsqueeze(0)).detach().numpy()  # Get the last sample\n",
    "test_predictions = minmax_scaler.inverse_transform(test_predictions)  # Reverse scaling transform\n",
    "test_predictions = test_predictions[0].tolist()\n",
    "test_predictions = inverse_pct_change(test_predictions, first_target)\n",
    "\n",
    "# Actual\n",
    "test_actual = y_val_tensors[-1].detach().numpy()  # Last actual y\n",
    "test_actual = minmax_scaler.inverse_transform(test_actual.reshape(1, -1))\n",
    "test_actual = test_actual[0].tolist()\n",
    "test_actual = inverse_pct_change(test_actual, first_target)\n",
    "\n",
    "plt.plot(test_actual, label=\"Actual\")\n",
    "plt.plot(test_predictions, label=\"Predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mas-thesis",
   "language": "python",
   "name": "mas-thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
